# -*- coding: utf-8 -*-
"""screening test DS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CqZJDk15-_r3fv_fswcizu4D1CiGmM0H
"""

# ✅ Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# ✅ Set paths to your files
json_path = '/content/drive/MyDrive/algoparams_from_ui.json.rtf'
csv_path = '/content/drive/MyDrive/iris.csv'

# ✅ Install required dependency
!pip install striprtf

# ✅ Import libraries
import json
import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, RobustScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectFromModel, SelectKBest, f_regression
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from striprtf.striprtf import rtf_to_text
import warnings
warnings.filterwarnings("ignore")

# ✅ Load JSON from RTF safely
def load_json_from_rtf(filepath):
    with open(filepath, 'r') as file:
        rtf_content = file.read()
    text = rtf_to_text(rtf_content)
    return json.loads(text)

# ✅ Get target and type
def get_target_info(config):
    target = config['design_state_data']['target']['target']
    prediction_type = config['design_state_data']['target']['prediction_type']
    return target, prediction_type

# ✅ Feature preprocessing excluding target
def create_preprocessor(config, target_col):
    features = config['design_state_data']['feature_handling']
    transformers = []
    for fname, fparams in features.items():
        if fparams['is_selected'] and fname != target_col:
            if fparams['feature_variable_type'] == 'numerical':
                strategy = fparams['feature_details']['impute_with']
                value = fparams['feature_details']['impute_value']
                if strategy == 'Average of values':
                    imputer = SimpleImputer(strategy='mean')
                else:
                    imputer = SimpleImputer(strategy='constant', fill_value=value)
                transformers.append((fname, Pipeline([
                    ('imputer', imputer),
                    ('scaler', RobustScaler())
                ]), [fname]))
            elif fparams['feature_variable_type'] == 'text':
                transformers.append((fname, OneHotEncoder(handle_unknown='ignore'), [fname]))
    return ColumnTransformer(transformers=transformers)

# ✅ Feature generation with numeric check
def generate_features(df, config):
    fg = config['design_state_data'].get('feature_generation', {})
    for pair in fg.get('linear_interactions', []):
        if all(col in df.columns and np.issubdtype(df[col].dtype, np.number) for col in pair):
            df[f'{pair[0]}_x_{pair[1]}'] = df[pair[0]] * df[pair[1]]
    for expr in fg.get('polynomial_interactions', []):
        try:
            c1, c2 = expr.split('/')
            if all(col in df.columns and np.issubdtype(df[col].dtype, np.number) for col in [c1, c2]):
                df[f'{c1}_div_{c2}'] = df[c1] / df[c2].replace(0, np.nan)
        except:
            continue
    for expr in fg.get('explicit_pairwise_interactions', []):
        try:
            c1, c2 = expr.split('/')
            if all(col in df.columns and np.issubdtype(df[col].dtype, np.number) for col in [c1, c2]):
                df[f'{c1}_by_{c2}'] = df[c1] * df[c2]
        except:
            continue
    return df

# ✅ Feature reduction
def create_feature_reducer(config):
    fr = config['design_state_data']['feature_reduction']
    method = fr['feature_reduction_method']
    k = int(fr['num_of_features_to_keep'])
    if method == 'PCA':
        return PCA(n_components=k)
    elif method == 'Tree-based':
        model = RandomForestRegressor(n_estimators=int(fr['num_of_trees']), max_depth=int(fr['depth_of_trees']))
        return SelectFromModel(model, max_features=k)
    elif method == 'Corr with Target':
        return SelectKBest(score_func=f_regression, k=k)
    elif method == 'No Reduction':
        return 'passthrough'
    else:
        raise ValueError(f"Unknown feature reduction method: {method}")

# ✅ Model selection
def get_models(config, prediction_type):
    algos = config['design_state_data']['algorithms']
    models = {}
    for name, params in algos.items():
        if not params['is_selected']:
            continue
        if prediction_type == 'Regression' and 'Regressor' in name:
            if name == 'RandomForestRegressor':
                model = RandomForestRegressor()
                grid = {
                    'model__n_estimators': list(range(params['min_trees'], params['max_trees'] + 1, 5)),
                    'model__max_depth': list(range(params['min_depth'], params['max_depth'] + 1, 5))
                }
            elif name == 'LinearRegression':
                model = LinearRegression()
                grid = {}
            elif name == 'RidgeRegression':
                model = Ridge()
                grid = {'model__alpha': np.linspace(params['min_regparam'], params['max_regparam'], 3)}
            elif name == 'LassoRegression':
                model = Lasso()
                grid = {'model__alpha': np.linspace(params['min_regparam'], params['max_regparam'], 3)}
            elif name == 'ElasticNetRegression':
                model = ElasticNet()
                grid = {
                    'model__alpha': np.linspace(params['min_regparam'], params['max_regparam'], 3),
                    'model__l1_ratio': np.linspace(params['min_elasticnet'], params['max_elasticnet'], 3)
                }
            else:
                continue
            models[name] = (model, grid)
    return models

# ✅ Training & evaluation
def train_and_evaluate(df, target, preprocessor, reducer, models):
    X = df.drop(columns=[target])
    y = df[target]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    for name, (model, param_grid) in models.items():
        print(f"\n--- Running Model: {name} ---")
        pipe = Pipeline([
            ('preprocess', preprocessor),
            ('reduce', reducer),
            ('model', model)
        ])
        grid = GridSearchCV(pipe, param_grid=param_grid, cv=5, n_jobs=-1)
        grid.fit(X_train, y_train)
        y_pred = grid.predict(X_test)
        print("Best Parameters:", grid.best_params_)
        print("R² Score:", r2_score(y_test, y_pred))
        print("MAE:", mean_absolute_error(y_test, y_pred))
        print("MSE:", mean_squared_error(y_test, y_pred))

# ✅ Run full pipeline
config = load_json_from_rtf(json_path)
df = pd.read_csv(csv_path)
df = generate_features(df, config)
target, prediction_type = get_target_info(config)
preprocessor = create_preprocessor(config, target)
reducer = create_feature_reducer(config)
models = get_models(config, prediction_type)
train_and_evaluate(df, target, preprocessor, reducer, models)

